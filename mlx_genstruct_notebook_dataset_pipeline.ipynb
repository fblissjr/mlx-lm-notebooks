{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74aea9f7",
   "metadata": {},
   "source": [
    "# MLX LOGIC ADDED\n",
    "Below is a port of Genstruct 7B notebook from NousResearch (https://huggingface.co/NousResearch/Genstruct-7B/blob/main/notebook.ipynb)\n",
    "\n",
    "I've ported the code to work with MLX, Apple's Silicon framework, and added an end to end pipeline that generates the data and saves it as both JSON and a hugging face dataset format.\n",
    "\n",
    "Great model, and really happy with how this works. It's using a csv file as the seed/'train' data for Genstruct. It then runs inference over each row, and outputs as a saved dataset that can then be used for something like qlora.\n",
    "\n",
    "Haven't gotten to the reward model part yet, so I've left that code as-is from NousResearch's HF repo. Plan to do Pairwise RM next, and then add in a qlora finetune section.\n",
    "\n",
    "...also had Claude 3 generate markdown comments on what's happening in the code in the style of a nature documentary. Enjoy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0f126",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "# Introducing Genstruct\n",
    "Generating high-quality synthetic instruction data is an important challenge. Standard approaches rely heavily on in-context learning and prompting of large language models to generate instruction pairs. This has limitations in terms of quality, diversity, and lack of explicit reasoning.\n",
    "\n",
    "Two previous methods aimed to improve upon this naive prompting approach:\n",
    "- Retrieval-augmented generation (RAG) pipelines convert passages from sources like Wikipedia into instructional pairs.\n",
    "- [Ada-Instruct](https://arxiv.org/abs/2310.04484) instead trains a custom model to generate instructions, rather than relying on prompting. This improves quality and diversity compared to prompting alone. Further, the authors of the Ada-Instruct paper found that training could be performed with as few as 10 examples.\n",
    "\n",
    "Genstruct is a new method that combines and extends these previous approaches. Like Ada-instruct, it is a custom trained model rather than relying on prompting. However, Ada-Instruct relies heavily on ungrounded generation, which can lead to hallucinations.  To mitigate this, Genstruct generates instructions based upon a user-provided context, like RAG methods.\n",
    "\n",
    "Additionally, Genstruct goes beyond prior work by focusing on the generation of complex questions and multi-step reasoning for each generated instruction pair, rather than just direct questions and responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf417800",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "## Generating instruction pairs\n",
    "Ada-Instruct is trained based on Mistral. Specifically, it is trained over the [MetaMath-Mistral-7B](meta-math/MetaMath-Mistral-7B) model, in order to improve reasoning with math-heavy topcs.\n",
    "\n",
    "Like any other Mistral model, it can be imported from Huggingface Hub as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f73db8",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "Genstruct works by generating instructions and answers from a user-provided context and title. It utilizes a custom prompt format, as in the following example:\n",
    "```\n",
    "[[[Title]]] p-value\n",
    "[[[Content]]] The p-value is used in the context of null hypothesis testing in order to quantify the statistical significance of a result, the result being the observed value of the chosen statistic T {\\displaystyle T}.[note 2] The lower the p-value is, the lower the probability of getting that result if the null hypothesis were true. A result is said to be statistically significant if it allows us to reject the null hypothesis. All other things being equal, smaller p-values are taken as stronger evidence against the null hypothesis.\n",
    "\n",
    "The following is an interaction between a user and an AI assistant that is related to the above text.\n",
    "\n",
    "[[[User]]]\n",
    "```\n",
    "\n",
    "The model then completes from `[[[User]]]`, generating an instruction and a response.\n",
    "\n",
    "\n",
    "To simplify its use, the Genstruct tokenizer includes a 'chat template'. It accepts a list containing a single dict, with members 'title' and 'content' - for the title and content of the context to generate from:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439295cb",
   "metadata": {},
   "source": [
    "...or in the style of a nature documentary, thanks to Claude:\n",
    "\n",
    "In the vast digital savannah, we observe a remarkable ritual unfold. A pack of alphanumeric hunters, led by the fearsome process_dataset function, prepare to stalk their prey - the elusive train_dataset.\n",
    "\n",
    "First, the hunters must load their weapons - the mighty model and tokenizer from the mlx_lm tribe. With a few deft keystrokes, they summon these formidable tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47250246",
   "metadata": {},
   "source": [
    "quick mlx test to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\n",
    "    \"./NousResearch_Genstruct-7B-mlx\",\n",
    ")\n",
    "\n",
    "msg = [\n",
    "    {\n",
    "        \"title\": \"the best title ever\",\n",
    "        \"content\": \"the best message ever\",\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.decode(tokenizer.apply_chat_template(msg))\n",
    "\n",
    "gen_text = generate(model, tokenizer, prompt, max_tokens=512, temp=0.6, verbose=True)\n",
    "\n",
    "# Split the generated text using the EOS token and take the first part\n",
    "gen_text_final = gen_text.split(tokenizer.eos_token, 1)[0]\n",
    "\n",
    "print(gen_text_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891496c",
   "metadata": {},
   "source": [
    "# Process Genstruct Input Data and Create Seed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c8e93",
   "metadata": {},
   "source": [
    "In the vast digital savannah, we observe a remarkable ritual unfold. A pack of alphanumeric hunters, led by the fearsome `process_dataset` function, prepare to stalk their prey - the elusive `train_dataset`.\n",
    "\n",
    "First, the hunters must load their weapons - the mighty `model` and `tokenizer` from the `mlx_lm` tribe. With a few deft keystrokes, they summon these formidable tools:\n",
    "\n",
    "```python\n",
    "model, tokenizer = load(\n",
    "    \"./NousResearch_Genstruct-7B-mlx\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f106a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\n",
    "    \"./NousResearch_Genstruct-7B-mlx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c500f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the train.csv file into a Hugging Face dataset\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"./data/train.csv\")\n",
    "\n",
    "# Shuffle the train dataset\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "# Save the shuffled train dataset as a parquet file\n",
    "train_dataset[\"train\"].save_to_disk(\"train_dataset\")\n",
    "\n",
    "# Load the shuffled train dataset from the parquet file\n",
    "train_dataset = Dataset.load_from_disk(\"train_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a929784",
   "metadata": {},
   "source": [
    "Now, the hunt begins in earnest. The `process_dataset` function, a veteran tracker, takes the `train_dataset` herd as its target, along with an optional `num_rows` parameter to limit the size of the cull.\n",
    "\n",
    "For each row in the herd, the hunters meticulously extract vital information - the `message` and `sentiment` - like experts reading the scent trails of their quarry.\n",
    "\n",
    "```python\n",
    "message = row[\"message\"]\n",
    "sentiment = row[\"sentiment\"]\n",
    "```\n",
    "\n",
    "A devious `msg` template is then crafted, laced with tantalizing metadata to lure the prey into a false sense of security.\n",
    "\n",
    "The hunters enlist the aid of the `tokenizer`, a trusty scout skilled in the art of prompts. With its guidance, they generate a scent trail that will lead their deadliest weapon - the `model` - straight to the hapless prey.\n",
    "\n",
    "```python\n",
    "prompt = tokenizer.decode(tokenizer.apply_chat_template(msg))\n",
    "```\n",
    "\n",
    "The hunt reaches its climax as the `generate` function unleashes the `model`, a ferocious beast capable of spinning coherent and contextually relevant discourse from the prompt.\n",
    "\n",
    "```python\n",
    "gen_text = generate(model, tokenizer, prompt, max_tokens=512, temp=0.6, verbose=True)\n",
    "```\n",
    "\n",
    "The `gen_text` is swiftly snared, with only the first part kept as the final, fatal blow.\n",
    "\n",
    "```python\n",
    "gen_text_final = gen_text.split(tokenizer.eos_token, 1)[0]\n",
    "```\n",
    "\n",
    "As each row falls, an `output_item` dictionary is assembled - a gruesome trophy with the original `message` as the \"instruction\" and the generated `gen_text_final` as the \"response\".\n",
    "\n",
    "One by one, these trophies are added to the `output_data` list, a growing monument to the hunters' prowess.\n",
    "\n",
    "When the herd has been sufficiently culled, the spoils are preserved - first as a `output_json` object serialized from the `output_data` list, and then as a `output_dataset` fashioned from the same grisly remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24f172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, num_rows=None):\n",
    "    \"\"\"\n",
    "    Processes a given dataset by iterating through each row, generating text using the specified model and tokenizer,\n",
    "    and creating an output dataset with the generated responses.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The input dataset to process.\n",
    "        num_rows (int, optional): The number of rows to process. If None, all rows will be processed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the output JSON string and the output dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the generated outputs\n",
    "    output_data = []\n",
    "\n",
    "    # Iterate through each row in the dataset\n",
    "    for i, row in enumerate(dataset):\n",
    "        # Check if the specified number of rows has been processed\n",
    "        if num_rows is not None and i >= num_rows:\n",
    "            break\n",
    "\n",
    "        # Extract the relevant information from the current row and strip whitespaces\n",
    "        message = row[\"message\"].strip()\n",
    "        sentiment = row[\"sentiment\"].strip()\n",
    "\n",
    "        # Log the message being processed\n",
    "        logger.info(f\"Processing message {i+1}: {message}\")\n",
    "\n",
    "        # Create the message template with metadata\n",
    "        msg = [\n",
    "            {\n",
    "                \"title\": f\"Conversation about X, <sentiment>{sentiment}</sentiment>\",\n",
    "                \"content\": f\"\"\"{message}\"\"\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Generate the prompt by applying the chat template to the message\n",
    "        prompt = tokenizer.decode(tokenizer.apply_chat_template(msg)).strip()\n",
    "\n",
    "        # Generate text using the specified model and tokenizer\n",
    "        gen_text = generate(\n",
    "            model, tokenizer, prompt, max_tokens=512, temp=0.6, verbose=False\n",
    "        )\n",
    "\n",
    "        # Split the generated text using the EOS token, take the first part, and strip whitespaces\n",
    "        gen_text_final = gen_text.split(tokenizer.eos_token, 1)[0].strip()\n",
    "\n",
    "        # Append the EOS token to the generated text\n",
    "        gen_text_final += tokenizer.eos_token\n",
    "\n",
    "        # Log the generated output\n",
    "        logger.info(f\"Generated output: {gen_text_final}\")\n",
    "\n",
    "        # Create a dictionary with the required schema\n",
    "        output_item = {\n",
    "            \"instruction\": f\"[[[User]]]{message}\",\n",
    "            \"response\": f\"[[[Content]]]{gen_text_final}\",\n",
    "        }\n",
    "\n",
    "        # Append the output item to the output_data list\n",
    "        output_data.append(output_item)\n",
    "\n",
    "    # Create a JSON object with the output data\n",
    "    output_json = json.dumps(output_data, indent=2)\n",
    "\n",
    "    # Create a Hugging Face dataset from the output data\n",
    "    output_dataset = Dataset.from_list(output_data)\n",
    "\n",
    "    # Return the output JSON and dataset\n",
    "    return output_json, output_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd744b8",
   "metadata": {},
   "source": [
    "```python\n",
    "output_json = json.dumps(output_data, indent=2)\n",
    "output_dataset = Dataset.from_list(output_data)\n",
    "```\n",
    "\n",
    "The `output_json` is cached in the \"output.json\" territory, while the `output_dataset` is staked out in the \"output_dataset\" domain - a warning to any future prey that dares to wander too close.\n",
    "\n",
    "And so, the circle of life in the coding kingdom continues, with the alphanumeric hunters ever vigilant, ever ready to pursue their next target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a specified number of rows (e.g., 10) from the shuffled train dataset (num_rows=x optional parameter for sampling only a select number of rows)\n",
    "output_json, output_dataset = process_dataset(train_dataset, num_rows=None)\n",
    "\n",
    "# Save the output JSON to a file\n",
    "with open(\"output.json\", \"w\") as f:\n",
    "    f.write(output_json)\n",
    "\n",
    "# Save the output dataset as a parquet file\n",
    "output_dataset.save_to_disk(\"output_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e12a9",
   "metadata": {},
   "source": [
    "# END MLX LOGIC - WORKING ON END TO END PAIRWISE RM AND QLORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b8d92",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "Generation can then be performed with `model.generate()`, as follows (or with vllm or whaatever other pipeline you prefer):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848af10",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "Note that the model is optimized for single-paragraph extracts from Wikipedia articles. You may have varying luck with other input types.\n",
    "\n",
    "## Filtering outputs using a reward model\n",
    "The model may occasionally generate incorrect or improperly formatted output - the likelihood of this can be reduced with clever sampling methods, such as rejection sampling using a reward model, or even simple regex filtering.\n",
    "\n",
    "For instance, we might consider `OpenAssistant/reward-model-deberta-v3-large-v2` as a reward model, and perform best-of-n sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93868ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[Title]]] p-value\n",
      "[[[Content]]] The p-value is used in the context of null hypothesis testing in order to quantify the statistical significance of a result, the result being the observed value of the chosen statistic T {\\displaystyle T}.[note 2] The lower the p-value is, the lower the probability of getting that result if the null hypothesis were true. A result is said to be statistically significant if it allows us to reject the null hypothesis. All other things being equal, smaller p-values are taken as stronger evidence against the null hypothesis.\n",
      "\n",
      "The following is an interaction between a user and an AI assistant that is related to the above text.\n",
      "\n",
      "[[[User]]]  Two medical procedures were compared by flipping 2 coins, procedure A assumed to be better and so it was labeled head, while procedure B was labeled as tail for a flip. The coins where then flipped 25 times, with the following results:[{'Tails', 12}, {'Heads', 13}]\n",
      "\n",
      "Which procedure had better results with statistical significance?\n",
      "[[[Assistant]]] The statistical significance of the outcomes between the two procedures can be assessed using the p-value, which represents the probability of obtaining results as extreme as, or more extreme than, those observed, if the null hypothesis is true.\n",
      "\n",
      "In this case, let's assume that the null hypothesis would suggest that there is no difference between the two procedures, so each one should result in heads or tails with approximately equal probability (assuming fair coins).\n",
      "\n",
      "To calculate the p-value, we can use the statistic T, which in this context could be any relevant statistic calculated from the data, such as the difference in the number of flips resulting in heads or tails. We want to find the p-value corresponding to the observed value of T when the data is Tails = 12, Heads\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "N = 4\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    ")\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"OpenAssistant/reward-model-deberta-v3-large-v2\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "def extract_pair(resp):\n",
    "    response = resp.split(\"[[[Content]]]\")[1]\n",
    "    inst, resp = resp.split(\"[[[User]]]\")[:2]\n",
    "    return inst.strip(), resp.strip()\n",
    "\n",
    "\n",
    "def score(resp):\n",
    "    inst, resp = extract_pair(resp.split(tokenizer.eos_token)[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = rm_tokenizer(inst, resp, return_tensors=\"pt\")\n",
    "        score = float(rm_model(**inputs).logits[0].cpu())\n",
    "        return score\n",
    "\n",
    "\n",
    "gens = tokenizer.batch_decode(\n",
    "    model.generate(inputs, max_new_tokens=256, num_return_sequences=N, do_sample=True)\n",
    ")\n",
    "print(max(gens, key=score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
