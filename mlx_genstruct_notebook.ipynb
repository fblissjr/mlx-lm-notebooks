{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe0f126",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "# Introducing Genstruct\n",
    "Generating high-quality synthetic instruction data is an important challenge. Standard approaches rely heavily on in-context learning and prompting of large language models to generate instruction pairs. This has limitations in terms of quality, diversity, and lack of explicit reasoning.\n",
    "\n",
    "Two previous methods aimed to improve upon this naive prompting approach:\n",
    "- Retrieval-augmented generation (RAG) pipelines convert passages from sources like Wikipedia into instructional pairs.\n",
    "- [Ada-Instruct](https://arxiv.org/abs/2310.04484) instead trains a custom model to generate instructions, rather than relying on prompting. This improves quality and diversity compared to prompting alone. Further, the authors of the Ada-Instruct paper found that training could be performed with as few as 10 examples.\n",
    "\n",
    "Genstruct is a new method that combines and extends these previous approaches. Like Ada-instruct, it is a custom trained model rather than relying on prompting. However, Ada-Instruct relies heavily on ungrounded generation, which can lead to hallucinations.  To mitigate this, Genstruct generates instructions based upon a user-provided context, like RAG methods.\n",
    "\n",
    "Additionally, Genstruct goes beyond prior work by focusing on the generation of complex questions and multi-step reasoning for each generated instruction pair, rather than just direct questions and responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf417800",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "## Generating instruction pairs\n",
    "Ada-Instruct is trained based on Mistral. Specifically, it is trained over the [MetaMath-Mistral-7B](meta-math/MetaMath-Mistral-7B) model, in order to improve reasoning with math-heavy topcs.\n",
    "\n",
    "Like any other Mistral model, it can be imported from Huggingface Hub as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aea9f7",
   "metadata": {},
   "source": [
    "# MLX LOGIC ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "804e7a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: [[[Title]]] Genstruct on Apple MLX\n",
      "[[[Content]]] MLX is an array framework for machine learning research on Apple silicon, brought to you by Apple machine learning research. MLX has a Python API that closely follows NumPy. MLX also has fully featured C++, C, and Swift APIs, which closely mirror the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers with APIs that closely follow PyTorch to simplify building more complex models. Composable function transformations: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization. Lazy computation: Computations in MLX are lazy. Arrays are only materialized when needed. Dynamic graph construction: Computation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive. Multi-device: Operations can run on any of the supported devices (currently the CPU and the GPU). Unified memory: A notable difference from MLX and other frameworks is the unified memory model. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data. MLX is designed by machine learning researchers for machine learning researchers. The framework is intended to be user-friendly, but still efficient to train and deploy models. The design of the framework itself is also conceptually simple. We intend to make it easy for researchers to extend and improve MLX with the goal of quickly exploring new ideas.(source: https://github.com/ml-explore/mlx)\n",
      "\n",
      "The following is an interaction between a user and an AI assistant that is related to the above text.\n",
      "\n",
      "[[[User]]] \n",
      "Two teams of researchers used different frameworks to train their models. Team A used MLX, while team B used framework B.\n",
      "Which team's model would more likely have higher-level packages?\n",
      "[[[Assistant]]] MLX, the framework employed by Team A, is specifically designed to facilitate machine learning research on Apple silicon. This framework, developed by Apple machine learning research, offers a Python API that closely follows NumPy and has fully featured C++, C, and Swift APIs mirroring the Python API. Additionally, MLX provides higher-level packages like mlx.nn and mlx.optimizers, which emulate the functionality of PyTorch, aiding in the simplified construction of more complex models.\n",
      "\n",
      "On the other hand, the details about framework B are not mentioned. Without any information about its features and functionalities, we cannot conclude whether it provides higher-level packages to simplify complex modeling.\n",
      "\n",
      "Given that MLX is designed specifically for machine learning research and has higher-level packages that emulate well-known frameworks like PyTorch, it is reasonable to assume that Team A's model would benefit from the increased simplicity and efficiency offered by these higher-level packages.\n",
      "\n",
      "Therefore, it is more likely that Team A's model would have higher-level packages.\n",
      "==========\n",
      "Prompt: 841.844 tokens-per-sec\n",
      "Generation: 29.132 tokens-per-sec\n",
      "Two teams of researchers used different frameworks to train their models. Team A used MLX, while team B used framework B.\n",
      "Which team's model would more likely have higher-level packages?\n",
      "[[[Assistant]]] MLX, the framework employed by Team A, is specifically designed to facilitate machine learning research on Apple silicon. This framework, developed by Apple machine learning research, offers a Python API that closely follows NumPy and has fully featured C++, C, and Swift APIs mirroring the Python API. Additionally, MLX provides higher-level packages like mlx.nn and mlx.optimizers, which emulate the functionality of PyTorch, aiding in the simplified construction of more complex models.\n",
      "\n",
      "On the other hand, the details about framework B are not mentioned. Without any information about its features and functionalities, we cannot conclude whether it provides higher-level packages to simplify complex modeling.\n",
      "\n",
      "Given that MLX is designed specifically for machine learning research and has higher-level packages that emulate well-known frameworks like PyTorch, it is reasonable to assume that Team A's model would benefit from the increased simplicity and efficiency offered by these higher-level packages.\n",
      "\n",
      "Therefore, it is more likely that Team A's model would have higher-level packages.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\n",
    "    \"./NousResearch_Genstruct-7B-mlx\",\n",
    ")\n",
    "\n",
    "msg = [\n",
    "    {\n",
    "        \"title\": \"Genstruct on Apple MLX \",\n",
    "        \"content\": \"\"\"MLX is an array framework for machine learning research on Apple silicon, brought to you by Apple machine learning research. MLX has a Python API that closely follows NumPy. MLX also has fully featured C++, C, and Swift APIs, which closely mirror the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers with APIs that closely follow PyTorch to simplify building more complex models. Composable function transformations: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization. Lazy computation: Computations in MLX are lazy. Arrays are only materialized when needed. Dynamic graph construction: Computation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive. Multi-device: Operations can run on any of the supported devices (currently the CPU and the GPU). Unified memory: A notable difference from MLX and other frameworks is the unified memory model. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data. MLX is designed by machine learning researchers for machine learning researchers. The framework is intended to be user-friendly, but still efficient to train and deploy models. The design of the framework itself is also conceptually simple. We intend to make it easy for researchers to extend and improve MLX with the goal of quickly exploring new ideas.(source: https://github.com/ml-explore/mlx)\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.decode(tokenizer.apply_chat_template(msg))\n",
    "\n",
    "gen_text = generate(model, tokenizer, prompt, max_tokens=512, temp=0.6, verbose=True)\n",
    "\n",
    "# Split the generated text using the EOS token and take the first part\n",
    "gen_text_final = gen_text.split(tokenizer.eos_token, 1)[0]\n",
    "\n",
    "print(gen_text_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f73db8",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "Genstruct works by generating instructions and answers from a user-provided context and title. It utilizes a custom prompt format, as in the following example:\n",
    "```\n",
    "[[[Title]]] p-value\n",
    "[[[Content]]] The p-value is used in the context of null hypothesis testing in order to quantify the statistical significance of a result, the result being the observed value of the chosen statistic T {\\displaystyle T}.[note 2] The lower the p-value is, the lower the probability of getting that result if the null hypothesis were true. A result is said to be statistically significant if it allows us to reject the null hypothesis. All other things being equal, smaller p-values are taken as stronger evidence against the null hypothesis.\n",
    "\n",
    "The following is an interaction between a user and an AI assistant that is related to the above text.\n",
    "\n",
    "[[[User]]]\n",
    "```\n",
    "\n",
    "The model then completes from `[[[User]]]`, generating an instruction and a response.\n",
    "\n",
    "\n",
    "To simplify its use, the Genstruct tokenizer includes a 'chat template'. It accepts a list containing a single dict, with members 'title' and 'content' - for the title and content of the context to generate from:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e12a9",
   "metadata": {},
   "source": [
    "# END MLX LOGIC - WORKING ON END TO END PAIRWISE RM AND QLORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b8d92",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "Generation can then be performed with `model.generate()`, as follows (or with vllm or whaatever other pipeline you prefer):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848af10",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "Note that the model is optimized for single-paragraph extracts from Wikipedia articles. You may have varying luck with other input types.\n",
    "\n",
    "## Filtering outputs using a reward model\n",
    "The model may occasionally generate incorrect or improperly formatted output - the likelihood of this can be reduced with clever sampling methods, such as rejection sampling using a reward model, or even simple regex filtering.\n",
    "\n",
    "For instance, we might consider `OpenAssistant/reward-model-deberta-v3-large-v2` as a reward model, and perform best-of-n sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93868ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[Title]]] p-value\n",
      "[[[Content]]] The p-value is used in the context of null hypothesis testing in order to quantify the statistical significance of a result, the result being the observed value of the chosen statistic T {\\displaystyle T}.[note 2] The lower the p-value is, the lower the probability of getting that result if the null hypothesis were true. A result is said to be statistically significant if it allows us to reject the null hypothesis. All other things being equal, smaller p-values are taken as stronger evidence against the null hypothesis.\n",
      "\n",
      "The following is an interaction between a user and an AI assistant that is related to the above text.\n",
      "\n",
      "[[[User]]]  Two medical procedures were compared by flipping 2 coins, procedure A assumed to be better and so it was labeled head, while procedure B was labeled as tail for a flip. The coins where then flipped 25 times, with the following results:[{'Tails', 12}, {'Heads', 13}]\n",
      "\n",
      "Which procedure had better results with statistical significance?\n",
      "[[[Assistant]]] The statistical significance of the outcomes between the two procedures can be assessed using the p-value, which represents the probability of obtaining results as extreme as, or more extreme than, those observed, if the null hypothesis is true.\n",
      "\n",
      "In this case, let's assume that the null hypothesis would suggest that there is no difference between the two procedures, so each one should result in heads or tails with approximately equal probability (assuming fair coins).\n",
      "\n",
      "To calculate the p-value, we can use the statistic T, which in this context could be any relevant statistic calculated from the data, such as the difference in the number of flips resulting in heads or tails. We want to find the p-value corresponding to the observed value of T when the data is Tails = 12, Heads\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "N = 4\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    ")\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"OpenAssistant/reward-model-deberta-v3-large-v2\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "def extract_pair(resp):\n",
    "    response = resp.split(\"[[[Content]]]\")[1]\n",
    "    inst, resp = resp.split(\"[[[User]]]\")[:2]\n",
    "    return inst.strip(), resp.strip()\n",
    "\n",
    "\n",
    "def score(resp):\n",
    "    inst, resp = extract_pair(resp.split(tokenizer.eos_token)[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = rm_tokenizer(inst, resp, return_tensors=\"pt\")\n",
    "        score = float(rm_model(**inputs).logits[0].cpu())\n",
    "        return score\n",
    "\n",
    "\n",
    "gens = tokenizer.batch_decode(\n",
    "    model.generate(inputs, max_new_tokens=256, num_return_sequences=N, do_sample=True)\n",
    ")\n",
    "print(max(gens, key=score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
